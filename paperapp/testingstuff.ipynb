{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc3cc328-afaf-445f-b54d-e1727b8ef492",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtextacy\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textacy'"
     ]
    }
   ],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b43bb161-9f3a-4d30-aef6-b16b46d5e170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import textacy\n",
    "from nltk.corpus import wordnet\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def fuzzy_match(keyword, text, threshold=80):\n",
    "    words = text.split()\n",
    "    matched_words = [word for word in words if fuzz.ratio(keyword, word) >= threshold]\n",
    "    return matched_words\n",
    "\n",
    "def modified_kwic(doc, keyword, window_width=5, ignore_case=True, threshold=80):\n",
    "    # Get plurals, synonyms, and fuzzy matches\n",
    "    plurals = {f\"{keyword}s\", f\"{keyword}es\"}\n",
    "    synonyms = get_synonyms(keyword)\n",
    "    fuzzy_matches = fuzzy_match(keyword, doc.text, threshold)\n",
    "\n",
    "    # Create a regular expression pattern for the search term\n",
    "    search_terms = set([keyword]).union(plurals).union(synonyms).union(fuzzy_matches)\n",
    "    search_terms_pattern = r\"|\".join(re.escape(term) for term in search_terms)\n",
    "\n",
    "    # Use the modified pattern in the kwic function\n",
    "    return textacy.extract.kwic(doc, search_terms_pattern, window_width, ignore_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "734d553d-3a98-48d8-9ee6-451f440fa7a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MokS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MokS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MokS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\MokS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from yellowbrick.text import TSNEVisualizer\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"notebook\", rc={\"font.size\":16,\n",
    "                                \"axes.titlesize\":20,\n",
    "                                \"axes.labelsize\":18})\n",
    "\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "from spellchecker import SpellChecker\n",
    "import contractions\n",
    "from num2words import num2words\n",
    "import emoji\n",
    "import random\n",
    "from textacy import extract\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from yellowbrick.text.freqdist import FreqDistVisualizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "from termcolor import colored\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from typing import List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1e6e8fc-78ee-4131-a205-8220461b0445",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Citation: Bruijns, B.; Knotter, J.;\\nTiggelaar...\n",
       "1     within conventional STR proﬁling since the bes...\n",
       "2     truly became commercially available. A systema...\n",
       "3     review are 8, 21, 12, and 2 for ‘ParaDNA,’ ‘Ra...\n",
       "4     samples, (h) perform well (sensitivity and sel...\n",
       "5     used as a presumptive test for samples with me...\n",
       "6     chemistry is available as ACE NGM SElect Expre...\n",
       "7     Samples such as cable ties, fabric, matchstick...\n",
       "8     Shackleton et al. used the AmpFlSTR1 NGMSElect...\n",
       "9     data generated showed that the workﬂow of Rapi...\n",
       "10    Figure 5. Schematic overview of the ANDE I-chi...\n",
       "11    ANDE 6C with the I-Chip to address the FBI’s Q...\n",
       "12    Figure 6. Overview of the MiDAS cartridge for ...\n",
       "13    \\nTable 1. Characteristics of the ParaDNA, Rap...\n",
       "14    system is more robust in terms of resistance t...\n",
       "15    C until use) near the testing area, while the ...\n",
       "16    \\n\u000f Time-to-result : although it is not (yet) ...\n",
       "17    , 26–37. [CrossRef]\\n4. Wiley, R.; Sage, K.; L...\n",
       "18    , 3424–3435. [CrossRef] [PubMed]\\n29. Romsos, ...\n",
       "19    , 179–188.\\n[CrossRef]\\n55. Watherston, J.; Wa...\n",
       "20    E.; et al. Developmental Validation of the DNA...\n",
       "dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "reader = PdfReader(\"paper.pdf\")\n",
    "text = []\n",
    "\n",
    "def visitor_body(text, cm, tm, font_dict, font_size):\n",
    "    y = tm[5]\n",
    "    if y > 50 and y < 720:\n",
    "        parts.append(text)\n",
    "\n",
    "for page in reader.pages:\n",
    "    parts = []\n",
    "    page.extract_text(visitor_text=visitor_body)\n",
    "    text.append(\"\".join(parts))\n",
    " \n",
    "    \n",
    "df_train=pd.Series(text)  \n",
    "df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "147e602c-dfd2-4dd7-a8b6-28cb6e729f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lower_case(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "def remove_spaces_tabs(text: str) -> str:\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def remove_punct(text: str) -> str:\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def remove_single_char(text: str) -> str:\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "def remove_html(text: str) -> str:\n",
    "    html = re.compile(r\"<.*?>\")\n",
    "    return html.sub(r\"\", text)\n",
    "\n",
    "def remove_url(text: str) -> str:\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "def remove_emoji(text: str) -> str:\n",
    "    # Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    return emoji_pattern.sub(r\"\", text)\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.update([\"et\", \"al\", \"et al\"]) # add custom stopwords\n",
    "    stop_words -= {\"no\", \"not\"} # remove custom stopwords\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    return \" \".join(filtered_sentence)\n",
    "\n",
    "def expand_contractions(text: str) -> str:\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def lemmatize_text_custom(text: str, lemmatizer) -> str:\n",
    "    wordnet_map = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV,\n",
    "    }\n",
    "\n",
    "    w_pos_tags = nltk.pos_tag(text.split())\n",
    "    lemmatized_output = \" \".join([lemmatizer.lemmatize(w, wordnet_map.get(pos[0], wordnet.NOUN)) for w, pos in w_pos_tags])\n",
    "    return lemmatized_output\n",
    "\n",
    "def stem_text_custom(text: str, stemmer) -> str:\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stemmed_output = \" \".join([stemmer.stem(w) for w in word_tokens])\n",
    "    return stemmed_output\n",
    "def correct_spelling(text: str) -> str:\n",
    "    spell = SpellChecker()\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            # if correction is none return the original word\n",
    "            if spell.correction(word) is not None:\n",
    "                corrected_text.append(spell.correction(word))\n",
    "            else:\n",
    "                corrected_text.append(word)\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare(text, pipeline, lemmatizer=None, stemmer=None):\n",
    "    tokens = text\n",
    "    for transform in pipeline:\n",
    "        # if lemmatize or stem function pass in, perform transformation\n",
    "        if transform.__name__ == \"lemmatize_text_custom\":\n",
    "            tokens = transform(tokens, lemmatizer)\n",
    "        elif transform.__name__ == \"stem_text_custom\":\n",
    "            tokens = transform(tokens, stemmer)\n",
    "        else:\n",
    "            tokens = transform(tokens)\n",
    "\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "020e7780-e28a-4632-8605-8b8689fe8945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [10:12<00:00, 29.19s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     citation bruins knotted tiggelaar systematic r...\n",
       "1     within conventional star prowling since best r...\n",
       "2     truly became commercially available systematic...\n",
       "3     review 8 21 12 2 i parade i i rapidity i i and...\n",
       "4     samples perform well sensitivity selectively n...\n",
       "5     used presumptive test samples medium high temp...\n",
       "6     chemistry available ace nom select express sam...\n",
       "7     samples cable ties fabric matchstick ziplock b...\n",
       "8     shackleton used ampflstr1 ngmselect express st...\n",
       "9     data generated showed workflow rapidity genera...\n",
       "10    figure 5 schematic overview and chip top view ...\n",
       "11    and ac chip address fbi i quality assurance st...\n",
       "12    figure 6 overview midas cartridge dna extracti...\n",
       "13    table 1 characteristics parade rapidity 200 ra...\n",
       "14    system robust terms resistance shocks upon tra...\n",
       "15    use near testing area chip and could stored ro...\n",
       "16    i timetoresult although not yet possible gener...\n",
       "17    26–37 crossed 4 wiley sage large budowle inter...\n",
       "18    3424–3435 crossed pulled 29 rooms el french al...\n",
       "19    179–188 crossed 55 watherston watson bruce lel...\n",
       "20    developmental validation dnascantm rapid dna a...\n",
       "dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = [\n",
    "    lower_case,\n",
    "    expand_contractions,\n",
    "    remove_spaces_tabs,\n",
    "    remove_url,\n",
    "    remove_punct,\n",
    "    remove_single_char,\n",
    "    remove_html,\n",
    "    remove_stopwords,\n",
    "    expand_contractions,\n",
    "    correct_spelling,\n",
    "    \n",
    "]\n",
    "\n",
    "df_train_clean = df_train.progress_apply(prepare, pipeline=pipeline)\n",
    "df_train_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b2c2158-61fb-4cfe-a35e-b225655a6fe5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     citation bruins knotted tiggelaar systematic r...\n",
       "1     within conventional star prowling since best r...\n",
       "2     truly became commercially available systematic...\n",
       "3     review 8 21 12 2 i parade i i rapidity i i and...\n",
       "4     samples perform well sensitivity selectively n...\n",
       "5     used presumptive test samples medium high temp...\n",
       "6     chemistry available ace nom select express sam...\n",
       "7     samples cable ties fabric matchstick ziplock b...\n",
       "8     shackleton used ampflstr1 ngmselect express st...\n",
       "9     data generated showed workflow rapidity genera...\n",
       "10    figure 5 schematic overview and chip top view ...\n",
       "11    and ac chip address fbi i quality assurance st...\n",
       "12    figure 6 overview midas cartridge dna extracti...\n",
       "13    table 1 characteristics parade rapidity 200 ra...\n",
       "14    system robust terms resistance shocks upon tra...\n",
       "15    use near testing area chip and could stored ro...\n",
       "16    i timetoresult although not yet possible gener...\n",
       "17    26–37 crossed 4 wiley sage large budowle inter...\n",
       "18    3424–3435 crossed pulled 29 rooms el french al...\n",
       "19    179–188 crossed 55 watherston watson bruce lel...\n",
       "20    developmental validation dnascantm rapid dna a...\n",
       "dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4542947-2d48-4dc1-aada-340a2c29a4ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rapid dna', 76),\n",
       " ('rapidity id', 57),\n",
       " ('dna analysis', 52),\n",
       " ('rapidity 200', 48),\n",
       " ('forensic sci', 42),\n",
       " ('globalfiler express', 39),\n",
       " ('parade rapidity', 35),\n",
       " ('nom select', 35),\n",
       " ('ampflstr nom', 33),\n",
       " ('select express', 33)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_ngrams(text: pd.Series, ngram: int =1, top_n: int =10) -> List[Tuple[str, int]]:\n",
    "    vec = CountVectorizer(ngram_range=(ngram, ngram), stop_words=\"english\").fit(text)\n",
    "    bag_of_words = vec.transform(text)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:top_n]\n",
    "\n",
    "get_top_ngrams(df_train_clean, ngram=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d497b7-2b40-44f1-9308-86dfe9ae7f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "import textacy\n",
    "from nltk.corpus import wordnet\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def search_keyword(text: str, keyword: str) -> List:\n",
    "    return list(extract.keyword_in_context(text, keyword, window_width=50, ignore_case=True))\n",
    "\n",
    "def search_keyword_in_df(text: pd.Series, keyword: str, n: int = 10) -> str:\n",
    "\n",
    "    # apply search keyword function to each row\n",
    "    kic = text.apply(search_keyword, keyword=keyword)\n",
    "    \n",
    "    # filter out empty lists\n",
    "    kic = kic[kic.apply(len) > 0]\n",
    "\n",
    "    # check if there are any results\n",
    "    if len(kic) > 0:\n",
    "        # check if n is greater than the number of samples\n",
    "        if len(kic) < n:\n",
    "            n = len(kic)\n",
    "        sample_list = kic.sample(n)\n",
    "\n",
    "        # print results\n",
    "        for sample in sample_list:\n",
    "            print(f\"{sample[0][0]} {colored(sample[0][1], 'red')} {sample[0][2]}\")\n",
    "    else:\n",
    "        print(f\"No samples found for {colored(keyword, 'red')}\")\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def fuzzy_match(keyword, text, threshold=80):\n",
    "    words = text.split()\n",
    "    matched_words = [word for word in words if fuzz.ratio(keyword, word) >= threshold]\n",
    "    return matched_words\n",
    "\n",
    "def modified_kwic(doc, keyword, window_width=5, ignore_case=True, threshold=80):\n",
    "    # Get plurals, synonyms, and fuzzy matches\n",
    "    plurals = {f\"{keyword}s\", f\"{keyword}es\"}\n",
    "    synonyms = get_synonyms(keyword)\n",
    "    fuzzy_matches = fuzzy_match(keyword, doc.text, threshold)\n",
    "\n",
    "    # Create a regular expression pattern for the search term\n",
    "    search_terms = set([keyword]).union(plurals).union(synonyms).union(fuzzy_matches)\n",
    "    search_terms_pattern = r\"|\".join(re.escape(term) for term in search_terms)\n",
    "\n",
    "    # Use the modified pattern in the kwic function\n",
    "    return textacy.extract.kwic(doc, search_terms_pattern, window_width, ignore_case)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
